import requests
from bs4 import BeautifulSoup
import os
import re
import time

# ì„¤ì •
# ì‚¬ìš©ìžë‹˜ì´ ì£¼ì‹  500ê°œì”© ë³´ê¸° ë§í¬ í™œìš©
SEARCH_URL = "https://www.data.go.kr/tcs/dss/selectDataSetList.do"
BASE_PARAMS = {
    "dType": "FILE",
    "keyword": "í¡ì—°êµ¬ì—­",
    "detailKeyword": "",
    "sort": "_score",
    "perPage": "100", # ì„œë²„ ì°¨ë‹¨ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ 100ê°œì”© ìª¼ê°œì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
}
SAVE_DIR = "data/raw_csv"
os.makedirs(SAVE_DIR, exist_ok=True)

class MasterDownloader:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.data.go.kr/"
        }

    def get_pks_from_page(self, page_num):
        """ê²€ìƒ‰ ê²°ê³¼ íŽ˜ì´ì§€ì—ì„œ publicDataPk ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
        params = BASE_PARAMS.copy()
        params["currentPage"] = str(page_num)
        
        print(f"ðŸ” ê²€ìƒ‰ ê²°ê³¼ {page_num}íŽ˜ì´ì§€ ë¶„ì„ ì¤‘...")
        resp = requests.get(SEARCH_URL, params=params, headers=self.headers)
        if resp.status_code != 200:
            print(f"âŒ íŽ˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {resp.status_code}")
            return []

        soup = BeautifulSoup(resp.text, 'html.parser')
        # íƒ€ì´í‹€ê³¼ PKê°€ í¬í•¨ëœ ë§í¬ë“¤ ì°¾ê¸°
        items = soup.select(".result-list > li")
        
        found_data = []
        for item in items:
            title_tag = item.select_one(".title")
            link_tag = item.select_one("dt > a")
            
            if title_tag and link_tag:
                title = title_tag.get_text(strip=True)
                # hrefì—ì„œ PK ì¶”ì¶œ (ì˜ˆ: /tcs/dss/selectFileDataDetailView.do?publicDataPk=15034166)
                pk_match = re.search(r"publicDataPk=(\d+)", link_tag['href'])
                if pk_match:
                    found_data.append({"title": title, "pk": pk_match.group(1)})
        
        return found_data

    def download_file(self, title, pk):
        """PKë¥¼ ì´ìš©í•´ ì‹¤ì œ CSV ë‹¤ìš´ë¡œë“œ"""
        # íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ìž ì œê±°
        clean_title = re.sub(r'[\\/*?:">|<]', "", title).replace(" ", "_")
        file_path = os.path.join(SAVE_DIR, f"{clean_title}.csv")
        
        if os.path.exists(file_path):
            print(f"â© ìŠ¤í‚µ (ì´ë¯¸ ì¡´ìž¬): {clean_title}")
            return

        download_url = f"https://www.data.go.kr/tcs/dss/fileDownload.do?publicDataPk={pk}"
        
        try:
            resp = requests.get(download_url, headers=self.headers, timeout=30)
            if resp.status_code == 200 and "html" not in resp.headers.get("Content-Type", ""):
                with open(file_path, 'wb') as f:
                    f.write(resp.content)
                print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {clean_title}")
                return True
            else:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ë°ì´í„° ì—†ìŒ): {clean_title}")
        except Exception as e:
            print(f"ðŸ”¥ ì—ëŸ¬ ë°œìƒ: {clean_title} - {e}")
        return False

def main():
    downloader = MasterDownloader()
    
    # 1. ìƒìœ„ 3íŽ˜ì´ì§€(300ê°œ ë°ì´í„°ì…‹)ë§Œ ë¨¼ì € ê³µëžµí•´ë´…ë‹ˆë‹¤.
    all_targets = []
    for p in range(1, 4):
        targets = downloader.get_pks_from_page(p)
        all_targets.extend(targets)
        time.sleep(1)

    print(f"\nðŸŽ¯ ì´ {len(all_targets)}ê°œì˜ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ ë°œê²¬!")
    
    # 2. ì¼ê´„ ë‹¤ìš´ë¡œë“œ ì‹œìž‘
    success = 0
    for target in all_targets:
        if downloader.download_file(target['title'], target['pk']):
            success += 1
        time.sleep(0.5) # ì„œë²„ ë§¤ë„ˆ ëŒ€ê¸° ì‹œê°„

    print(f"\nâœ¨ ìž‘ì—… ì™„ë£Œ! {success}ê°œì˜ CSV íŒŒì¼ì´ '{SAVE_DIR}'ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
