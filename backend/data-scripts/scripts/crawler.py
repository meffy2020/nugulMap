import requests
from bs4 import BeautifulSoup
import os
import re
import time

# ì„¤ì •
SEARCH_URL = "https://www.data.go.kr/tcs/dss/selectDataSetList.do"
BASE_DOMAIN = "https://www.data.go.kr"
DOWNLOAD_DIR = "backend/data-scripts/data/raw_csv"

class CSVCrawler:
    def __init__(self, keyword="í¡ì—°êµ¬ì—­"):
        self.keyword = keyword
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        os.makedirs(DOWNLOAD_DIR, exist_ok=True)

    def get_dataset_list(self, page=1):
        """ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶”ì¶œ"""
        params = {
            "keyword": self.keyword,
            "dataType": "FILE", # íŒŒì¼ ë°ì´í„°ë§Œ
            "currentPage": page,
            "perPage": 20
        }
        print(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ {page}í˜ì´ì§€ ë¶„ì„ ì¤‘...")
        resp = requests.get(SEARCH_URL, params=params, headers=self.headers)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
        items = soup.select(".result-list > li")
        dataset_links = []
        for item in items:
            title = item.select_one(".title").text.strip()
            # ìƒì„¸ í˜ì´ì§€ ë§í¬ ì¶”ì¶œ
            link_tag = item.select_one("dt > a")
            if link_tag:
                dataset_links.append({
                    "title": title,
                    "url": BASE_DOMAIN + link_tag['href']
                })
        return dataset_links

    def download_csv(self, dataset_info):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹¤ì œ CSV ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì•„ ë‹¤ìš´ë¡œë“œ"""
        try:
            resp = requests.get(dataset_info['url'], headers=self.headers)
            # data-file-id ë˜ëŠ” ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì˜ ID ì¶”ì¶œ (ì •ê·œì‹ ì‚¬ìš©)
            file_id_match = re.search(r"fn_fileDownload\('(\d+)'\)", resp.text)
            
            if file_id_match:
                file_id = file_id_match.group(1)
                download_url = f"https://www.data.go.kr/tcs/dss/fileDownload.do?dataNm={file_id}" # ê°€ìƒ ì£¼ì†Œ
                
                # ì‹¤ì œ ê³µê³µë°ì´í„° í¬í„¸ì€ POST/GET ë°©ì‹ì´ ë³µì¡í•˜ë¯€ë¡œ 
                # ì—¬ê¸°ì„œëŠ” ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ë‹¤ì´ë ‰íŠ¸ ë‹¤ìš´ë¡œë“œ íŒ¨í„´ì„ ì‹œë„í•˜ê±°ë‚˜ 
                # ë°ì´í„°ì…‹ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
                
                # ì‹¤ì œë¡œëŠ” ìƒì„¸ í˜ì´ì§€ì˜ 'CSV' ë²„íŠ¼ì˜ hrefë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì •í™•í•¨
                soup = BeautifulSoup(resp.text, 'html.parser')
                csv_btn = soup.find("a", string=re.compile("CSV"))
                
                if csv_btn:
                    # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë¡œì§ì€ ì„¸ì…˜ ìœ ì§€ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
                    print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {dataset_info['title']}")
                    # ... (ë‹¤ìš´ë¡œë“œ ë¡œì§ ìƒëµ - êµ¬ì¡°ë§Œ ì œì‹œ)
                    return True
            return False
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {dataset_info['title']} - {e}")
            return False

def main():
    crawler = CSVCrawler()
    # 1. 1~3í˜ì´ì§€ê¹Œì§€ í›‘ìœ¼ë©° CSV ë°ì´í„°ì…‹ ì°¾ê¸°
    all_links = []
    for p in range(1, 4):
        links = crawler.get_dataset_list(p)
        if not links: break
        all_links.extend(links)
    
    print(f"\nâœ¨ ì´ {len(all_links)}ê°œì˜ í¡ì—°êµ¬ì—­ ë°ì´í„°ì…‹ ë°œê²¬!")
    print("ì´ ë¦¬ìŠ¤íŠ¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ CSVë¥¼ ìë™ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    # ğŸ’¡ íŒ: ì‹¤ì œ ê³µê³µë°ì´í„° í¬í„¸ì€ ë³´ì•ˆ ë°©í™”ë²½ì´ ê°•ë ¥í•´ì„œ ìŒ© íŒŒì´ì¬ìœ¼ë¡œ 
    # ëŒ€ëŸ‰ ë‹¤ìš´ë¡œë“œë¥¼ ë§‰ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. 
    # ê·¸ë˜ì„œ ê°€ì¥ ì¢‹ì€ 'í¬ë¡¤ë§' ëŒ€ì•ˆì€ [ê²€ìƒ‰ ê²°ê³¼ì˜ CSV ë§í¬ ë¦¬ìŠ¤íŠ¸]ë§Œ ë½‘ì•„ì„œ
    # ì‚¬ìš©ìë‹˜ê»˜ 'ì¼ê´„ ë‹¤ìš´ë¡œë“œ ëª…ë ¹ì–´'ë¥¼ ë“œë¦¬ëŠ” ê²ƒì…ë‹ˆë‹¤.

if __name__ == "__main__":
    main()
